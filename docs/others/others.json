[
  {
    "path": "others/2023-01-24-basic-Git-Commands-cheatsheet/",
    "title": "Basic Git Commands for Biginners",
    "description": "This post is inspired by the error obtained when pushing the changes made to the Github repository which is \"refusing to merge the unrelated histories\".",
    "author": [],
    "date": "2023-01-23",
    "categories": [
      "Version Control System",
      "GitHub",
      "Gitbash"
    ],
    "contents": "\r\n\r\nContents\r\nOverview\r\nSources of the error\r\nHow to Fix the Error\r\nHow to avoid the Error in Future\r\nBasic Commands in Git\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n RPubs blog posts\r\n Twitter\r\n\r\nOverview\r\nAs I have mentioned above from the description of this short post, I have encountered the error called refusing to merge unrelated histories, when I tried to push the changes made to my project. With this, I will discuss the possible sources, how to fix such error and how to avoid it in future project. In addition,we will also get a chance to provide the cheatsheet for basic commands in Git.\r\nSources of the error\r\nCommonly, this error might be sourced from having branches with unrelated history base. This means that one is starting the branches independently of each other.For example if you start a new Git project on a local machine (i.e Rproject with Git version control) and then connect it to a remote GitHub branch (cloned from GitHub), these branches would have different history bases.\r\nThe only exception is when one of the branches has no commits in it. In this case, they should merge without a problem. Otherwise, we’ll get the refusing to merge unrelated histories like in the following example:\r\n\r\n$ git pull origin main\r\n#output\r\nfatal: refusing to merge unrelated histories\r\n\r\nHow to Fix the Error\r\nTo fix the obtained error, you need to use the option –-allow-unrelated-histories after the git pull  command.\r\nremote: is the remote repository URL or its short-name origin\r\nbranch: is the branch name that we’d like to merge\r\nExample\r\n\r\n$ git pull origin main --allow-unrelated-histories\r\n\r\n\r\nThe –-allow-unrelated-histories option will tell the Git that we allow merging branches with no common history base, which then should finish the merge without errors.\r\n\r\nHow to avoid the Error in Future\r\nTypically, it’s not the best practice to create a local repository branch independently of the remote repository. A more reliable way is to download the remote repository to the local machine using the git clone command.\r\n\r\n $ git clone <repo_url>\r\n\r\nThis way, we copy the repository from the remote server, and the commit history base remains the same for both remote and local branches.\r\nBasic Commands in Git\r\nHere below are some of basic commands of Git\r\ngit config: used to set the email and name of the author\r\ngit init: used to start a new repository\r\ngit add + file name: adds file to the staging area\r\ngit commit -m “message”: used to commit file\r\ngit status: list all the files that have to be committed\r\ngit log: used to view the commit history\r\ngit giff: shows different between two files\r\ngit branch: lists all the local branches in the repository\r\ngit branch + branch-name: create a new branch\r\ngit branch -d + branch-name: used to delete the branch\r\ngit checkout + branch-name: used to switch from one branch to another\r\ngit push + remote+branch-name: used to save commits in the remote repository.\r\ngit pull + repository link: incorporates changes from a remote repository into the current branch.\r\ngit clone: download existing code from the remote repository\r\ngit stash: used to remove the uncommitted files temporarily\r\ngit reset undoes all the commits after the specified commit and preserves the change locally\r\ngit rm + file-name: removes file from the directory.\r\nAs conclusion, the above commands are being executed in Git bash or in Terminal.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-28T11:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "others/2022-11-12-interpretation-of-ML-outputs/",
    "title": "Explaining AI Visualization(ModelStudio)",
    "description": "This post is inspired by the model and post written by Matt Dancho and being repricated on other data set to ultimately interprete the outputs of ML nodel in visual and most understandable way. \n'Machine learning is great, until you have to explain it': Matt Dancho has said,",
    "author": [],
    "date": "2022-11-12",
    "categories": [
      "Machine Learning",
      "modelStudio",
      "tidyverse",
      "DALEX"
    ],
    "contents": "\r\n\r\nContents\r\nOverview\r\nSteps and Workflow\r\nStep1: Loading data and Libaries\r\nStep2: Make a Predictive Model\r\nStep3: Make Explainer\r\nStep4: Run modelStudio\r\nAcknowledgement\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n RPubs blog posts\r\n modelStudio\r\n\r\nOverview\r\nThis small blog post is aiming to visual explain the output of trained and won machine learning model through the usage of modelstudio library created by Matt Dancho and being hosted in CRAN.\r\nmodelStudio is a new R package that makes it easy to interactively explain machine learning models using state-of-the-art techniques like Shapley Values, Break Down plots, and Partial Dependence (Matt Dancho, 2022).\r\nSteps and Workflow\r\nIn this blog, we will learn how to make my 4 most important Explainable AI plots:\r\nFeature Importance\r\nBreak Down Plot\r\nShapley Values\r\nPartial Dependence\r\nStep1: Loading data and Libaries\r\nLoad the libraries and Data\r\nLoad Libraries: Load modelStudio , DALEX, tidyverse and tidymodels.\r\nImport Data: We’re using the mpg dataset that comes with ggplot2.\r\n\r\n\r\n\r\nlibrary(modelStudio)\r\nlibrary(tidyverse)\r\nlibrary(DALEX)\r\nlibrary(tidymodels)\r\n\r\ndata<- rio::import(\"C:/Users/jmurera/Desktop/Blog/myblog/data/Breast_cancer_data.csv\")\r\n\r\ndata_tbl<-data%>% mutate_if(is.integer, as.factor) %>% as.tibble()\r\n\r\n\r\nThe data to be used looks like this\r\n\r\n\r\nlibrary(flextable)\r\nft <- flextable(head(data_tbl))\r\nft <- autofit(ft)\r\n\r\nfor(i in 1:1){\r\n  flextable_to_rmd(ft)\r\n}\r\n\r\nmean_radiusmean_texturemean_perimetermean_areamean_smoothnessdiagnosis17.9910.38122.801,001.00.11840020.5717.77132.901,326.00.08474019.6921.25130.001,203.00.10960011.4220.3877.58386.10.14250020.2914.34135.101,297.00.10030012.4515.7082.57477.10.127800\r\n\r\nWe want to understand how breast cancer diagnosis status can be estimated based on the remaining 5 columns.\r\nStep2: Make a Predictive Model\r\nThe best way to understand what affects cancer diagnosis decision is to build a predictive model (and then explain it). Let’s build an xgboost model using the tidymodels ecosystem. If you’ve never heard of Tidymodels, it’s like Scikit Learn for R and CARET ecosystem.\r\nSelect Model Type: We use the boost_tree() function to establish that we are making a Boosted Tree\r\nSet the Mode: Using set_mode() we select classification because we are predicting a class label of patient.\r\nSet the Engine: Next we use set_engine() to tell Tidymodels to use the xgboost library.\r\nFit the Model: This performs a simple training of the model to fit each of the 5 predictors to the target diagnosis. Note that we did not perform cross-validation, hyperparameter tuning, or any advanced concepts as they are beyond the scope of this tutorial.\r\n\r\n\r\nfit_xgboost<- boost_tree(learn_rate = 0.3) %>% \r\n              set_mode(\"classification\") %>% \r\n              set_engine(\"xgboost\") %>% \r\n              fit(diagnosis~.,data=data_tbl)\r\n#fit_xgboost\r\n\r\n\r\nStep3: Make Explainer\r\nWith above predictive model, we are ready to create an explainer. In basic terms, an explainer is a consistent and unified way to explain predictive models. The explainer can accept many different model types like:\r\nTidymodels\r\nmlr3\r\nH2O\r\nPython Scikit Learn Models\r\nAnd it returns the explanation results from the model in a consistent format for investigation.\r\nNow, below is the code to create the explainer.\r\n\r\n\r\n#---------Explainer\r\nexplainer<-DALEX::explain(\r\n  model = fit_xgboost,\r\n  data= data_tbl[,-6],\r\n  y=as.numeric(unlist(data_tbl[,6])),\r\n  label = \"Extreme Gradient Boosting Machine (XGBoost)\"\r\n)\r\n\r\nPreparation of a new explainer is initiated\r\n-> model label : Extreme Gradient Boosting Machine (XGBoost)\r\n-> data : 569 rows 5 cols\r\n-> data : tibble converted into a data.frame\r\n-> target variable : 569 values\r\n-> predict function : yhat.model_fit will be used ( default )\r\n-> predicted values : No value for predict function target column. ( default )\r\n-> model_info : package parsnip , ver. 1.0.3 , task classification ( default )\r\n-> predicted values : numerical, min = 0.00912416 , mean = 0.6253965 , max = 0.9911299\r\n-> residual function : difference between y and yhat ( default )\r\n-> residuals : numerical, min = 0.1481095 , mean = 1.00202 , max = 1.515258\r\nA new explainer has been created!\r\n\r\nStep4: Run modelStudio\r\n\r\n\r\nmodStudio<-modelStudio::modelStudio(explainer = explainer)\r\n#modStudio\r\n\r\n\r\nAcknowledgement\r\n\r\nWe intensively express our recognition to the developers of modelStudio who are Hubert Baniecki and Przemyslaw Biecek. This package is part of the Dr. Why ecosystem of R packages, which are a collection of tools for Visual Exploration, Explanation and Debugging of Predictive Models. Thank you for everything you do, we owe you much respect to simplify our work.\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2023-02-28T11:30:52+02:00",
    "input_file": {}
  }
]
